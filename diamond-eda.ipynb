{
  "cells": [
    {
      "metadata": {
        "_uuid": "8dafa7d449c3303d5ad6bd3a0f9bd843afbf9e27",
        "_cell_guid": "f071c85c-02df-4b1c-b7e0-dd1edd5ae617"
      },
      "cell_type": "markdown",
      "source": "# **Exploring the Diamond Dataset** #\n\n***Author: Daniel Marrama***\n\n## Introduction ##\nThis kernel begins my real journey as a Kaggler as I attempt to explore a dataset and create visualizations using **Python**. I am still relatively new to data science, so this will be a learning process for me as well. I picked this dataset to start because there is only one csv file to work with and there isn't a crazy amount of features for each sample. I've been reading through other Kaggle kernels for inspiration and hope to improve my skills by doing my own. Any feedback is appreciated and I thank you in advance.\n\nTo start, we will load in the data, take a peek at it, clean it if necessary and then perform some basic analysis in order to get some possible insight."
    },
    {
      "metadata": {
        "_uuid": "571cefa57d8300d09b2308cf196cff746d323146",
        "_cell_guid": "1b1a426b-e36e-48e0-bb8c-b708f83a5ba5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_palette(\"husl\")\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read in the data using pd.read_csv\n# The first column is the index column, so we will set the index to that column\ndata = pd.read_csv('../input/diamonds.csv',index_col=0)\ndata.sample(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0e3f1d4bc3c5ec1f07451cc2cb27e67dc3d017cf",
        "_cell_guid": "ae5bd610-85a2-4177-af46-b16423d0288b"
      },
      "cell_type": "markdown",
      "source": "Let's look to see if we have any missing values at all and we will deal with those appropriately."
    },
    {
      "metadata": {
        "_uuid": "5621f0d5ce70d6c389f33e432aaa62aa703d4f1c",
        "_cell_guid": "aa16846f-3700-4cf7-a330-14d2f044b5d3",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "87418a6a72cc72da4de1377c310f5f00b40c6936",
        "_cell_guid": "d7ec5eb6-abfb-4a57-afff-f2688615e62c"
      },
      "cell_type": "markdown",
      "source": "Wow, there appears to be no missing data points, this is another reason why this is a good dataset to start with as a beginner. Of course, in the real world, most of the time we will not have datasets like this, so it is good to practice with other data that needs to be cleaned up.\n\nTo simplify this data set a bit more, let's combine the 'x', 'y' and 'z' columns into a 'volume' column. We'll also remove any outliers above 1000mm^3."
    },
    {
      "metadata": {
        "_uuid": "1bb352a260b83812fd6a4fb54983f9769f9d56c6",
        "_cell_guid": "6ab96d88-8a8a-4940-bde5-927913b04b84",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data['volume'] = data['x']*data['y']*data['z']\ndata.drop(['x','y','z'],axis=1,inplace=True)\ndata = data[data['volume']<1000]\ndata.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b9c792e30aeda60336118c56f76585fb34d0173",
        "_cell_guid": "7d6cd1bb-8ca1-4d20-b57e-a7ea28e7bcd9"
      },
      "cell_type": "markdown",
      "source": "Next, let's look at some distributions of the carat, depth, price and volume columns using matplotlib. We will use seaborn later, just to be able to experiment with both libraries at bit."
    },
    {
      "metadata": {
        "_uuid": "b5a33c554e695cc8673455a56ba1239c3c5ff570",
        "_cell_guid": "5e110f7c-1ffe-4893-93b4-5aee5b3b2d3f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=[12,12])\n\n# First subplot showing the diamond carat weight distribution\nplt.subplot(221)\nplt.hist(data['carat'],bins=20,color='b')\nplt.xlabel('Carat Weight')\nplt.ylabel('Frequency')\nplt.title('Distribution of Diamond Carat Weight')\n\n# Second subplot showing the diamond depth distribution\nplt.subplot(222)\nplt.hist(data['depth'],bins=20,color='r')\nplt.xlabel('Diamond Depth (%)')\nplt.ylabel('Frequency')\nplt.title('Distribution of Diamond Depth')\n\n# Third subplot showing the diamond price distribution\nplt.subplot(223)\nplt.hist(data['price'],bins=20,color='g')\nplt.xlabel('Price in USD')\nplt.ylabel('Frequency')\nplt.title('Distribution of Diamond Price')\n\n# Fourth subplot showing the diamond volume distribution\nplt.subplot(224)\nplt.hist(data['volume'],bins=20,color='m')\nplt.xlabel('Volume in mm cubed')\nplt.ylabel('Frequency')\nplt.title('Distribution of Diamond Volume')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f3ec8e352b4597b37ae77a5207e25380001be3a6",
        "_cell_guid": "7360a97b-7dd5-458b-b7fd-c8370df93f6e"
      },
      "cell_type": "markdown",
      "source": "The depth percentage of the diamond is the depth from the table to the cutlet (point of the diamond) relative to the width and length of the diamond. It makes sense that the depth percentage is around 60% for most diamonds because otherwise, it would be strange to have a deep depth or a shallow depth and it would be hard to seat a diamond in a ring whose depth is small.\n\nIt also makes sense that the volume and carat weight are almost overlapped equally.\n\nLet's look at some comparison's with some features of the diamond and with the price of it using the seaborn library."
    },
    {
      "metadata": {
        "_uuid": "e16971c4bfc79ccb8ec6689857212165545633fc",
        "_cell_guid": "06520407-561c-49db-b3be-73650f916a45",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.regplot(x = 'carat', y = 'price', data=data, ax = saxis[0,0])\nsns.regplot(x = 'volume', y = 'price', data=data, ax = saxis[0,1])\n\n# Order the plots from worst to best\nsns.barplot(x = 'cut', y = 'price', order=['Fair','Good','Very Good','Premium','Ideal'], data=data, ax = saxis[1,0])\nsns.barplot(x = 'color', y = 'price', order=['J','I','H','G','F','E','D'], data=data, ax = saxis[1,1])\nsns.barplot(x = 'clarity', y = 'price', order=['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF'], data=data, ax = saxis[1,2])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ad7984e4f721927a21bec437a0a4e2c145a41133",
        "_cell_guid": "865aca81-d11c-47d7-8d48-20d9407ce761"
      },
      "cell_type": "markdown",
      "source": "Generally, the price increases as carat weight and volume increase and this makes sense. However, it is strange that as the qualities of cut, color and clarity of a diamond increase, the price does not increase. This may be because diamonds high in quality in one of these attributes may be poor quality in another.\n\nBecause of this, what we will do next might be a stretch, but let's normalize the cut, color and clarity columns and then add them all up to achieve a \"diamond score\". Then we will graph the score compared with the price and see if there is an increase based on the score.\n\nFirst, we will have to convert these columns to integers instead of the strings that are already within them."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "47088a8d66848e6742079d6071d41e66e7e32036",
        "_cell_guid": "6c2a7c36-ea27-4c3f-9d50-abac5903b1ad",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# These look very ugly, but I'm not sure how else to do this\n# I tried using the LabelEncoder method from scikit-learn preprocessing\n# but I'm not sure how to label them based on order\ndata['cut'] = data['cut'].apply(lambda x: 1 if x=='Fair' else(2 if x=='Good' \n                                           else(3 if x=='Very Good' \n                                           else(4 if x=='Premium' else 5))))\n\ndata['color'] = data['color'].apply(lambda x: 1 if x=='J' else(2 if x=='I'\n                                          else(3 if x=='H'\n                                          else(4 if x=='G'\n                                          else(5 if x=='F'\n                                          else(6 if x=='E' else 7))))))\n\ndata['clarity'] = data['clarity'].apply(lambda x: 1 if x=='I1' else(2 if x=='SI2'\n                                          else(3 if x=='SI1'\n                                          else(4 if x=='VS2'\n                                          else(5 if x=='VS1'\n                                          else(6 if x=='WS2'\n                                          else 7 if x=='WS1' else 8))))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47494affbbcc121afc4aa34ae6faab78503dc2c7",
        "_cell_guid": "2b82652d-1b46-422e-9516-a67b06c99700"
      },
      "cell_type": "markdown",
      "source": "Next, let's normalize the data using the MinMaxScaler method from scikitlearn.preprocessing. Then we will create a 'diamond score' column based on the cut quality, color and clarity of the diamond. We will then graph the diamond score against the price and see if there is a positive trend."
    },
    {
      "metadata": {
        "_uuid": "0f4ca1f1408ff3f99037fac00335fda3fc93e09b",
        "_cell_guid": "4af0b216-7a2f-483f-bbe8-f61e7ea773af",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "scaler = MinMaxScaler()\ndata[['cut','color','clarity']] = scaler.fit_transform(data[['cut','color','clarity']])\n\ndata['diamond score'] = data['cut'] + data['color'] + data['clarity']\n\nsns.regplot(x = 'diamond score', y = 'price', data=data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8635f1b482e950f6d9a669427c885435a4429118",
        "_cell_guid": "7923753b-2836-44e4-a5e8-9cc9f11f1c03"
      },
      "cell_type": "markdown",
      "source": "This is very interesting. The data has a lot of variance, but the best fit line appears to be negative. I wonder if there are some organizations that overprice their diamonds and it is affecting the trend of the data. \n\nLet's do an obligatory correlation heatmap just to see if there is a strong correlation with any features that aren't obvious now that they are all in numerical format."
    },
    {
      "metadata": {
        "_uuid": "be4bd3b0b2422ec43db2bf6bfdda743fecb1bdc3",
        "_cell_guid": "7f63aed5-649a-4700-b9ae-c97a9371223f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(12, 12))\ncorrelation = data.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\nsns.heatmap(correlation, vmax=1, annot=True,square=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2852b5e520659d49d2e98cc4c20d2f9a29181282",
        "_cell_guid": "f15ff08a-a820-485a-b135-6a91622195e0"
      },
      "cell_type": "markdown",
      "source": "Hmm, there doesn't seem to be anything too strong that we wouldn't already have guessed. Anyhow, let's move on and do some basic MLA on the dataset just for the fun of it. We will split the data up and use the latter end of it as our 'test' set. We will then use the first set to train our model and the test data to predict prices after we've dropped the 'price' column."
    },
    {
      "metadata": {
        "_uuid": "7b6e5e58f9fd55e4d854687713dc3d44e6bb2328",
        "_kg_hide-output": false,
        "_kg_hide-input": false,
        "_cell_guid": "773b1d8b-6451-4cc2-a813-139492e15c30",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test_data = data.iloc[-round(len(data)*.1):].copy()\ndata.drop(data.index[-round(len(data)*.1):],inplace=True)\ntest_data.drop('price',1,inplace=True)\nprint(data.shape)\nprint(test_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "ebb6c3e5e815e6e1dcc11ddcb26e10db6b9a5d92",
        "_cell_guid": "bcf2c4e1-67d8-4cf6-bd99-cd9035fd4081"
      },
      "cell_type": "markdown",
      "source": "Let's load in a few ML algorithms from scikit-learn to run on this dataset. We'll include linear regression (for single variable regression) and ridge regression, LASSO and Elastic Net (for multivariable regression). I'm not too well versed in how to set the parameters for these models (I will be learning this soon), so for now I'm going to stick very closely to the default settings."
    },
    {
      "metadata": {
        "_uuid": "d58ae6ce494dba8ff0d53af8b94ad6833770fc9f",
        "_cell_guid": "420d5d73-0e12-42c8-8c5e-a96495995084",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.cross_validation import train_test_split\n\nX = data.drop(['price'],1)\ny = data['price']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n\nlinear_regression = LinearRegression()\nlinear_regression.fit(X_train,y_train)\nprint('Linear regression accuracy: ', linear_regression.score(X_test,y_test))\n\nridge = Ridge(normalize=True)\nridge.fit(X_train,y_train)\nprint('Ridge regression accuracy: ',ridge.score(X_test,y_test))\n\nlasso = Lasso(normalize=True)\nlasso.fit(X_train,y_train)\nprint('Lasso regression accuracy: ',ridge.score(X_test,y_test))\n\nelastic_net = ElasticNet()\nelastic_net.fit(X_train,y_train)\nprint('Elastic net accuracy: ',elastic_net.score(X_test,y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d57fb62e5b1bd65815ba581bd21decfe23bc2453",
        "_cell_guid": "746dcc83-3a28-4e96-a704-4266af09b3b4"
      },
      "cell_type": "markdown",
      "source": "Just for fun, I'm gonna plot the price prediction for each model against the diamond score and maybe we'll see a large difference graphically for each model."
    },
    {
      "metadata": {
        "_uuid": "a50842229f665dd19eb653ea6c43237357c63d35",
        "_cell_guid": "cbee5db4-ff83-4b5c-963f-22da9ecf7d0a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=[12,12])\n\n# Linear regression model\nplt.subplot(221)\nplt.scatter(test_data['diamond score'],linear_regression.predict(test_data),color='lightcoral')\nplt.ylim(0,8000)\nplt.xlabel('Diamond Score')\nplt.ylabel('Price in USD')\nplt.title('Linear Regression Model')\n\n# Ridge regression model\nplt.subplot(222)\nplt.scatter(test_data['diamond score'],ridge.predict(test_data),color='royalblue')\nplt.ylim(0,8000)\nplt.xlabel('Diamond Score')\nplt.ylabel('Price in USD')\nplt.title('Ridge Regression Model')\n\n# Lasso regression model\nplt.subplot(223)\nplt.scatter(test_data['diamond score'],lasso.predict(test_data),color='lightgreen')\nplt.ylim(0,8000)\nplt.xlabel('Diamond Score')\nplt.ylabel('Price in USD')\nplt.title('Lasso Regression Model')\n\n# Elastic net model\nplt.subplot(224)\nplt.scatter(test_data['diamond score'],elastic_net.predict(test_data),color='orange')\nplt.ylim(0,8000)\nplt.xlabel('Diamond Score')\nplt.ylabel('Price in USD')\nplt.title('Elastic Net Model')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aa5a19a992c802c343d4471bfb5d1391fb44c50e",
        "_cell_guid": "0d1a69c8-fd64-48b4-9caf-4921e1beb6f9"
      },
      "cell_type": "markdown",
      "source": "It seems the models actually look pretty different and they clearly differ in slope. I'll have to learn about each algorithm a bit more to see why this is.\n\nThat's all I have for now. If anyone has suggestions about what to add to this kernel or if you identify any mistakes, please let me know. I'm here to learn and like I said in the beginning, any and all feedback is welcome. :)\n\nThanks for spending the time to read through it!"
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "mimetype": "text/x-python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "nbconvert_exporter": "python",
      "version": "3.6.3",
      "name": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}